{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 5256,
     "status": "ok",
     "timestamp": 1700438131276,
     "user": {
      "displayName": "Honglai Peng",
      "userId": "09422365991702136955"
     },
     "user_tz": 300
    },
    "id": "k_TOpAPf71tx"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "import os\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23346,
     "status": "ok",
     "timestamp": 1700438154620,
     "user": {
      "displayName": "Honglai Peng",
      "userId": "09422365991702136955"
     },
     "user_tz": 300
    },
    "id": "v4FTRDNS9zd3",
    "outputId": "a6195e07-b4b1-4855-b4cf-dfa95b9c65ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "/content/drive/MyDrive/Project11_FakeNewsDetection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "# Set Working Directory\n",
    "%cd /content/drive/MyDrive/Project11_FakeNewsDetection\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 11343,
     "status": "ok",
     "timestamp": 1700438165961,
     "user": {
      "displayName": "Honglai Peng",
      "userId": "09422365991702136955"
     },
     "user_tz": 300
    },
    "id": "wz-tyP37AMmj"
   },
   "outputs": [],
   "source": [
    "# Load the trained Doc2Vec, TF-IDF, StandardScaler, and model\n",
    "doc2vec_model = Doc2Vec.load('doc2vec_model_path')\n",
    "with open('tfidf_vectorizer.pkl', 'rb') as file:\n",
    "    tfidf_vectorizer = pickle.load(file)\n",
    "with open('scaler.pkl', 'rb') as file:\n",
    "    scaler = pickle.load(file)\n",
    "model = load_model('text_classification_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1700438165962,
     "user": {
      "displayName": "Honglai Peng",
      "userId": "09422365991702136955"
     },
     "user_tz": 300
    },
    "id": "EaaaGbFnI1pc"
   },
   "outputs": [],
   "source": [
    "# Preprocess text function\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        return word_tokenize(text.lower())\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1700438165962,
     "user": {
      "displayName": "Honglai Peng",
      "userId": "09422365991702136955"
     },
     "user_tz": 300
    },
    "id": "6-Vm7mWbI5-Y"
   },
   "outputs": [],
   "source": [
    "def predict_label(text):\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return 0  # Mark as real if it's not text\n",
    "    processed_text = preprocess_text(text)\n",
    "    doc_vec = doc2vec_model.infer_vector(processed_text)\n",
    "    tfidf_vec = tfidf_vectorizer.transform([' '.join(processed_text)]).toarray()[0]\n",
    "    combined_features = np.hstack((doc_vec, tfidf_vec))\n",
    "    scaled_features = scaler.transform([combined_features])\n",
    "    prediction = model.predict(scaled_features, verbose=0).round().astype(int).flatten()[0]\n",
    "    return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 5590,
     "status": "ok",
     "timestamp": 1700438171542,
     "user": {
      "displayName": "Honglai Peng",
      "userId": "09422365991702136955"
     },
     "user_tz": 300
    },
    "id": "EBNBbP7kLeD1"
   },
   "outputs": [],
   "source": [
    "#Load Data\n",
    "df = pd.read_csv(\"updated_filtered_atlanta_restaurant_reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1124632,
     "status": "ok",
     "timestamp": 1700439296165,
     "user": {
      "displayName": "Honglai Peng",
      "userId": "09422365991702136955"
     },
     "user_tz": 300
    },
    "id": "iiZwhuUoI-6i",
    "outputId": "61fe4931-3e15-458d-99b2-81bf9bde4f86"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks:   5%|▌         | 1/20 [00:43<13:55, 43.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 741 saved as data_new_lstm_2/741.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks:  10%|█         | 2/20 [01:20<11:49, 39.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 742 saved as data_new_lstm_2/742.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks:  15%|█▌        | 3/20 [01:37<08:19, 29.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 743 saved as data_new_lstm_2/743.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks:  20%|██        | 4/20 [02:31<10:27, 39.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 744 saved as data_new_lstm_2/744.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks:  25%|██▌       | 5/20 [02:55<08:22, 33.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 745 saved as data_new_lstm_2/745.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks:  30%|███       | 6/20 [03:23<07:23, 31.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 746 saved as data_new_lstm_2/746.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks:  35%|███▌      | 7/20 [04:16<08:21, 38.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 747 saved as data_new_lstm_2/747.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks:  40%|████      | 8/20 [04:59<07:59, 39.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 748 saved as data_new_lstm_2/748.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks:  45%|████▌     | 9/20 [05:08<05:34, 30.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 749 saved as data_new_lstm_2/749.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks:  50%|█████     | 10/20 [06:02<06:15, 37.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 750 saved as data_new_lstm_2/750.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks:  55%|█████▌    | 11/20 [06:25<04:59, 33.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 751 saved as data_new_lstm_2/751.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks:  60%|██████    | 12/20 [07:03<04:36, 34.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 752 saved as data_new_lstm_2/752.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks:  65%|██████▌   | 13/20 [07:39<04:06, 35.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 753 saved as data_new_lstm_2/753.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks:  70%|███████   | 14/20 [08:09<03:21, 33.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 754 saved as data_new_lstm_2/754.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks:  75%|███████▌  | 15/20 [08:35<02:35, 31.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 755 saved as data_new_lstm_2/755.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks:  80%|████████  | 16/20 [09:06<02:05, 31.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 756 saved as data_new_lstm_2/756.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks:  85%|████████▌ | 17/20 [09:25<01:22, 27.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 757 saved as data_new_lstm_2/757.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks:  90%|█████████ | 18/20 [09:57<00:57, 28.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 758 saved as data_new_lstm_2/758.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks:  95%|█████████▌| 19/20 [10:18<00:26, 26.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 759 saved as data_new_lstm_2/759.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks: 100%|██████████| 20/20 [10:41<00:00, 32.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 760 saved as data_new_lstm_2/760.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final complete file saved as data_new_lstm_2/complete_file.csv\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# Split dataframe into chunks\n",
    "chunk_size = 1000\n",
    "chunks = np.array_split(df, len(df) // chunk_size)\n",
    "\n",
    "# # Create 'data' directory if it doesn't exist\n",
    "data_directory = \"data_new_lstm_2\"\n",
    "if not os.path.exists(data_directory):\n",
    "    os.makedirs(data_directory)\n",
    "\n",
    "# # Process each chunk and save\n",
    "# processed_chunks = []\n",
    "# for i, chunk in enumerate(tqdm(chunks, desc=\"Processing Chunks\"), start=1):\n",
    "#     processed_chunk = chunk.copy()\n",
    "#     processed_chunk['predicted_label'] = processed_chunk['text'].apply(predict_label)\n",
    "\n",
    "#     # Save processed chunk to CSV\n",
    "#     filename = os.path.join(data_directory, f\"{i}.csv\")\n",
    "#     processed_chunk.to_csv(filename, index=False)\n",
    "#     processed_chunks.append(processed_chunk)\n",
    "\n",
    "#     # Print the filename of the saved chunk\n",
    "#     print(f\"Chunk {i} saved as {filename}\")\n",
    "\n",
    "# # Concatenate all processed chunks and save the final file\n",
    "# final_df = pd.concat(processed_chunks)\n",
    "# final_filename = os.path.join(data_directory, \"complete_file.csv\")\n",
    "# final_df.to_csv(final_filename, index=False)\n",
    "# print(f\"Final complete file saved as {final_filename}\")\n",
    "\n",
    "# Start processing from chunk 323\n",
    "start_chunk_index = 740\n",
    "processed_chunks = []\n",
    "\n",
    "for i, chunk in enumerate(tqdm(chunks[start_chunk_index:], desc=\"Processing Chunks\"), start=start_chunk_index + 1):\n",
    "    processed_chunk = chunk.copy()\n",
    "    processed_chunk['predicted_label'] = processed_chunk['text'].apply(predict_label)\n",
    "\n",
    "    filename = os.path.join(data_directory, f\"{i}.csv\")\n",
    "    if not os.path.exists(filename):\n",
    "        processed_chunk.to_csv(filename, index=False)\n",
    "        print(f\"Chunk {i} saved as {filename}\")\n",
    "    else:\n",
    "        print(f\"Chunk {i} already processed. Skipping.\")\n",
    "    processed_chunks.append(processed_chunk)\n",
    "\n",
    "# Concatenate all newly processed chunks with previously processed ones\n",
    "new_processed_chunks = pd.concat(processed_chunks)\n",
    "existing_processed_chunks = pd.concat([pd.read_csv(os.path.join(data_directory, f\"{j}.csv\")) for j in range(1, start_chunk_index + 1)])\n",
    "final_df = pd.concat([existing_processed_chunks, new_processed_chunks])\n",
    "\n",
    "# Save the final complete file\n",
    "final_filename = os.path.join(data_directory, \"complete_file.csv\")\n",
    "final_df.to_csv(final_filename, index=False)\n",
    "\n",
    "print(f\"Final complete file saved as {final_filename}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOL/Ri4Wj66XZGSkNTNvAog",
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
